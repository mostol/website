<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/style.css">
  <!-- Grab title from the page data and dump it here -->
  <title>Adventures in Graphs | Jackson Mostoller</title>
</head>
<body>
  <nav id="navbar">
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/notes/">Open Notes</a></li>
  </ul>
</nav>
  <!-- Grab the content from the page data and dump it here, and mark it as safe -->
  <!-- Safe docs: https://mozilla.github.io/nunjucks/templating.html#safe -->
  <p>A handful of months ago my MS program advisor dropped a link to IARAI's
<a href="https://github.com/iarai/science4cast">2021Science4Cast competition</a> in the
program's forums. The competition's task is to turn ML-soothesayer and predict
future connections between machine learning topics within academic publications (e.g.
will &quot;Deep Learning&quot; and &quot;Ordinary Differential Equations&quot; show up in the
same paper in the next three years?). I poked through the <a href="https://github.com/iarai/science4cast/blob/main/Tutorial/tutorial.ipynb">example solution</a>,
which precomputed some metrics for each pair of topics in the training set
and fed them to a simple three-layer neural network and thought, &quot;I could beat
that, no problem!&quot;</p>
<p>Did I beat that? Yes! Was it &quot;no problem&quot;? No! Here's what I tried, failed at, and learned.</p>
<h2>(Not) Everything's a Vision Problem</h2>
<p>I could have sworn I remembered Jeremy Howard saying something along the lines of, &quot;Almost anything can be treated as a computer vision problem.&quot; It turns out I can't find any record of that ever happening now, but there <em>is</em> a section highlighting that &quot;Image Recognizers Can Tackle Non-Image Tasks&quot; in the <a href="https://github.com/fastai/fastbook/blob/master/01_intro.ipynb">first chapter of fast.ai's <em>fastbook</em></a>. Either way, my first instinct was to convert the dataset into a computer vision problem by conjuring up some kind of image-based representation of the dataset.</p>
<p>While the Science4Cast dataset contains millions of data points, the structure of this data is almost as simple as you can get—each entry only has:</p>
<ol>
<li>A topic represented as a number (semantic information about topics was withheld)</li>
<li>Another topic, represented by its respective number-label, which became connected to the first topic.</li>
<li>The time at which these topics were linked together</li>
</ol>
<p>And that's everything!</p>
<p>The competition's tutorial solution relied on calculating the degree of each topic/node, as well as counting common neighbors shared with other nodes. This seemed like a reasonable approach, because with so little information, and no other context, each node is essentially <em>defined</em> by its neighbors, so it's a good idea to focus on those relationships. At this point, I realized that there already exists an image-like representation of this kind of information—an <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>. Obviously image-based solutions aren't everyone's cup of tea...but wasn't this some low-hanging fruit? I could just take the world's most ubiquitous graph representation, divvy it up into per-node versions, shove them two-by-two through a neural network (because they already <em>are</em> a matrices), and train up a model that is leveraging <em>all</em> of the data available, all at once.</p>

</body>
</html>