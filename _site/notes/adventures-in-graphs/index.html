<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/style.css">
  <!-- Grab title from the page data and dump it here -->
  <title>Adventures in Graphs, Part 1: (Not) Everything&#39;s a Vision Problem | Jackson Mostoller</title>
</head>
<body>
  <nav id="navbar">
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/about/">About</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/notes/">Open Notes</a></li>
  </ul>
</nav>
  <!-- Grab the content from the page data and dump it here, and mark it as safe -->
  <!-- Safe docs: https://mozilla.github.io/nunjucks/templating.html#safe -->
  <p>This last year, IARAI ran a <a href="https://github.com/iarai/science4cast">2021 Science4Cast competition</a>, with the task of turning research-soothsayer and predicting what topics will be linked together through future machine learning research publications (e.g. will &quot;Deep Learning&quot; and &quot;Ordinary Differential Equations&quot; show up in the same paper in the next three years?). I poked through the <a href="https://github.com/iarai/science4cast/blob/main/Tutorial/tutorial.ipynb">example solution</a>, which precomputed some metrics for each pair of topics in the training set and fed them to a simple three-layer neural network and thought, &quot;I could beat that, no problem!&quot;</p>
<p>Did I beat it? Yep! Was it &quot;no problem&quot;? Nope! In fact, I attempted two completely different approaches, and one of them didn't even beat the tutorial solution provided as a straightforward example. And that's the one I'll describe here, because even if it wasn't successful, it was an interesting process! (To me, at least.) Here's how it went.</p>
<h2>Sight setting</h2>
<p>Since the end goal of the competition is to be able to take in two topics and predict whether or not they will connect in the future, the most intuitive approach to me seemed to be feeding a representation of each topic into a neural network, and training that network to classify the outcome for us.</p>
<p>I know that there's a neuropsychologically-rooted understanding of deep neural networks, but I am not a neuropsychologist, so my best intuitive grasp of how neural nets work is rooted in the <a href="https://wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a> and functions in their most basic form, which is &quot;you put a thing in, and you get a thing out&quot;. If we can clearly define what we're putting in and what we want to get out, then we can set up a model that takes thing <code>A</code> in and spits thing <code>B</code> out and let our training algorithms optimize that process ‚ú®<em>mathemagically</em>‚ú® to do it with data it hasn't seen before. Thinking about a deep learning model this way, while maye not the most rigorous or complex approach, has the added benefit of letting me focus on single idea, which helps me avoid getting bogged down in less-important implementation details or do something that's accidentally incompatible with the competition task.</p>
<p>The details in between are flexible, but here was my mantra for this approach:</p>
<ol>
<li>The model must take take in representations of <em>two</em> nodes, and</li>
<li>The model must tell me, yes or no, <em>Will these two nodes be connected in 3 years?</em></li>
</ol>
<p>For the sake of flexibility, I decided that, while the competition expects me to predict results for three years from the present, that number shouldn't necessarily be baked in to the fundamental design of the model, so I dubbed this the &quot;n-Years Model&quot;, where <code>n = 3</code> in this case.</p>
<h2>Everything's a vision problem?</h2>
<p>So, how do we set up a model that takes in two node representations and spits out <em>the future</em>?</p>
<p>I could have sworn I remembered <a href="https://fast.ai">Jeremy Howard</a> saying something along the lines of, &quot;Almost anything can be treated as a computer vision problem.&quot; It turns out I can't find any record of that ever happening now, but there <em>is</em> a section highlighting how &quot;Image Recognizers Can Tackle Non-Image Tasks&quot; in the <a href="https://github.com/fastai/fastbook/blob/master/01_intro.ipynb">first chapter of fast.ai's <em>fastbook</em></a>. Either way, my first instinct was to convert the dataset into a computer vision problem by conjuring up some kind of image-based representation of the dataset, and then just using plain old image-classifying techniques to get it to make the right predictions. So the game plan is to feed the model an <em>image</em> representing two nodes, and have it classify whether or not they will be connected in three years.</p>
<h3>Step 1: Imagification</h3>
<p>The competition's tutorial solution relied on calculating the degree of each topic/node, as well as counting common neighbors shared with other nodes. This seemed like a reasonable approach, because with so little information, and no other context, each node is essentially <em>defined</em> by its neighbors‚Äîso it's not a bad idea to focus on those relationships.</p>
<p>Since each topic is semantically defined by its connections, can our image representation be pulled straight from a complete description of all of a node's relationships? Sure! For starters, we can convert our dataset into an <a href="https://en.wikipedia.org/wiki/Adjacency_list">adjacency list</a>, the sometimes less-popular but often more storage space-friendly sibling of the <a href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>.</p>
<p>An adjacency list actually lends itself really well to creating images from its information, because images are really just arrays or tensors, and in PyTorch (<a href="https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing">and NumPy</a>, where PyTorch got the idea from), tensors have &quot;fancy&quot; indexing of arrays using other arrays. Why is that handy? Because we can theoretically take a list of non-adjacent node id's, use them as the indices to select from another existing array, and set them to whatever value(s) we choose. In other words, if we have a node's adjacency list, we can use that directly as an index to represent that node's neighbors as <code>1</code> (or some other value) in a tensor, while leaving all other values <code>0</code>.</p>
<pre><code class="language-python3">node0_neighbors = np.array([1, 4, 6, 7, 9]) # A made-up list of node 0's neighbor nodes.

# Create a &quot;base&quot; of zeros
base = torch.zeros(64000) # Each of the 64000 nodes gets a spot.

base[node0_neighbors] = 1 # Now, all of node 0's neighbors are `1`, and everything else is 0.
</code></pre>
<p>Getting a full-graph adjacency list to use for this isn't too hard, but there <em>is</em> one caveat: our list is going to need to retain information about not just node connectivity, but also time. Remember, our model is supposed to tell us whether two nodes will be connected in <em>n</em> years‚Äîso in order to provide the model with ground-truth data to train on, we need to know about nodes' connectivity three years from whatever we feed the model. We'll get to how we could incorporate that to use during training, but here's the creation of our adjacency list, making sure we retain the temporal info:</p>
<pre><code class="language-python3"># Using the built-in `defaultdict` from Python's `collections`
# lets us easily build a dict iteratively without having
# to know the final state its elements from the start:

from collections import defaultdict

adj_list = defaultdict(list)

# The `graph` is just an array listing all of
# the graph's edges, formatted as:
#   node1 | node2 | time

for edge in graph:
    n1,n2,t = edge

    adj_list[n1].append([n2,t])
    adj_list[n2].append([n1,t])

# To enable fancy indexing (and speed up other processes),
# we'll turn these lists into arrays by way of tensors. A 
# little hacky? Yes. But it was the fastest method I could find! üòÖ

for key,value in arr_dict.items():
    adj_list[k] = np.asarray(torch.LongTensor(value).T)
</code></pre>
<p>Now, if we want to create an &quot;image&quot; for a node's connectivity, we can use the little process from above and get this:</p>
<pre><code class="language-python3"># If there are ~64000 nodes, then 255 x 255 could be a big enough image size
image_shape = (255,255)

base = torch.zeros(np.prod(image_shape))

# Generate the image for node 37
edges = adj_list[37][0] # We have to index the 0th item: [0]: connected nodes, [1]: connection times
base[edges] = 1 # Set selected elements to 1

# We'll even make it look image-y by making it a square shape:
image = base.reshape(img_shape)
</code></pre>
<p>Neat! Now to deal with that <em>time</em> element. But wait‚Ä¶what's that? Something's derailing us‚Äî<strong>*A wild ambition appears!*</strong> üòÆ</p>
<h3>Tangent 1: <em>k</em>-hop neighbors</h3>
<p>You know what's (ostensibly) better than 1-hop neighborhoods at representing nodes and supplying substantial information to train on? <em>2-hop</em> neighborhoods! Right now, each of our images we feed to the model are poised to simply be basic representations of a node's immediate neighbors‚Äîbut what if we also included the <em>neighbors of those neighbors</em> at a given point in time in our image? Wouldn't the sheer density and magnitude of additional relevant information‚Äîthe unreasonable effectiveness of more data!‚Äî<em>blow the socks off</em> of the tutorial model?! üò±ü§Ø It turns out that it did not. But I went through the trouble of making 2nd order neighbors work, anyways, so here we are.</p>
<p>In theory, it shouldn't be too hard to get the second order neighbors for a node. We already have all of the neighbors in our adjacency list, so we can just index the adjacency list of each of a node's neighbors, and we're good! Here's a simple way to do that:</p>
<pre><code class="language-python3">src_node = 1 # Finding node 1's 2nd order neighbors

[adj_list[neighbor][0] for neighbor in adj_list[src_node][0]]
</code></pre>
<p>That's great, but we're also going to need to use the temporal data from all of these neighbors‚Äîas noted above, we need to restrict our view of the graph to certain time constraints, which means we can't just index <em>every</em> neighbor. Instead, we'll want something more like this:</p>
<pre><code class="language-python3">src_node = 1
t = 8000 # Set time cutoff to day 8000

# The third indexing selector on each of these only indexes
# the locations where the expression is `True`, i.e.
# wherever the time array is less than `t`.
[adj_list[neighbor][0][adj_list[neighbor][1] &lt; t]
 for neighbor in
 adj_list[src_node][0][adj_list[src_node][1] &lt; t]]
</code></pre>
<p>There is also a <a href="#altered-matrix-multiplication">much more convoluted way</a> of doing this that entails implementing an altered matrix multiplication for a sparsely-described adjacency matrix, leveraging the fact that <a href="https://arxiv.org/abs/1207.3122">the square of an adjacency matrix represents the number of walks of length two from one node to another</a>. I originally opted for this approach out of fear that Python's iteration would be too slow with the method above for practical use during training, but after testing it out while writing this post‚Ä¶the above list comprehension is actually faster. Here's my final function to get a node's first and second order neighbors (with built-in functionality for getting up to <em>k</em> total layers of neighbors, although exponential list growth with this approach makes any <em>k</em> &gt; 2 a bit problematic):</p>
<pre><code class="language-python3">def get_neighbors(adj_list, node, t=None, k=2):
    neighbors_list = []
    nbrs = [node]
    if t is None:
        for _ in range(k):
            # If our node *has* no neighbors, we'll get an error we need to handle.
            try:
                nbrs = np.concatenate([adj_list[node][0] for node in nbrs])
            except (IndexError, ValueError):
                nbrs = np.array([],dtype=np.int64)

            neighbors_list.append(nbrs)
    else:
        for _ in range(k):
            try:
                nbrs = np.concatenate([adj_list[node][0][adj_list[node][1] &lt; t]
                                       for node in nbrs])
            except (IndexError, ValueError):
                nbrs = np.array([],dtype=np.int64)

            neighbors_list.append(nbrs)

    return neighbors_list
</code></pre>
<p>This returns a list of first order neighbors, followed by second neighbors (and up to <em>k</em>th order neighbors if we really wanted).</p>
<h3>Step 2: Temporal label-wrangling</h3>
<p>If you think about it, we don't really even need <em>any</em> information about the time that previous connections were made when we're doing predictions; it ought to be enough to know the connectivity of each node right now and whip up an image. But if we remember the second pillar of my model mantra:</p>
<blockquote>
<p>The model must tell me, yes or no, <em>Will these two nodes be connected in 3 years?</em></p>
</blockquote>
<p>in order to train the model, we're going to need to pass the model node representations and also <em>tell</em> it if those nodes happened to get connected within 3 years, so that it has enough context to actually answer the question.</p>
<p>In order to have lots of possible samples to train off of, it seems like a good idea to be able to pass the model <em>any</em> two nodes at <em>any</em> given point in time, and to get a yes/no as to whether they will be linked in three years. I found it easiest to think about this in terms of &quot;do these nodes connect at any point at all?&quot; and then handle the timing part appropriately after figuring that out‚Äîthis way, we can maximize the number of actual connections we feed the model so that training is more balanced, instead of feeding it nodes-that-won't-connect 99.8% of the time. There are a few possibilities here:</p>
<ol>
<li>One (or both) of the nodes has <em>no</em> connections at the given time. A blank slate is not particularly meaningful here, so it might be best to just avoid or ignore this case.</li>
<li>The nodes both have neighbors, but they <em>never</em> connect to one another. In this case, any time point is as good as another, so we can pick whichever one we want to feed the model. <strong>BUT!</strong> We have to be careful about the maximum timestamp we're willing to offer. If I feed the model two nodes as they exist on January 1, 2017, but my data only goes as far as 2019, then I know that they haven't been linked <em>yet</em>, but I <em>do not know</em> if they will connect within 3 years because it hasn't <em>been</em> three years yet. So in this case, we can input the nodes as they are at any date, <em>as long as</em> that date is beyond <em>n</em> years of our most up-to-date data.</li>
<li>The nodes do connect at some point. Keeping in mind that we want to maximize our already-scarce supply of positive samples, we probably want to make sure we pick a date that puts these cases to good use. If two topics are linked in March 2009, for example, I don't want to give the model their representations as they exist in April 2003, because based on the April 2003 images, the answer to the question, <em>Will these two nodes be connected in 3 years?</em> is <code>no</code>. This is why focusing on connection status over dates is handy‚Äîin these instances, we can make sure we pass representations that are within <em>n</em> years of connection into our model for training so we have as many positive samples as possible.</li>
</ol>
<p>With all that forethought out of the way, we can dive into setting up and training the model!</p>
<h2>Transforms &amp; training</h2>
<p>For training, I used fast.ai, leveraging its <a href="https://docs.fast.ai/tutorial.siamese.html#Using-the-mid-level-API">mid-level API</a> to work nicely with the above conditions. I decided that, from a user standpoint, the most ideal and intuitive way to interact with the model would be to simply pass in two node id's and let the model either train on that info or predict on in. Here's how we can make that happen with fast.ai's <a href="https://fastcore.fast.ai/transform#Transform"><code>Transform</code></a>s.</p>
<p>A <code>Transform</code> is, at the most basic level, just a function: it takes an input and <em>transforms</em> it into something else. (You might say a <code>Transform</code> is a <em>function in disguise</em>.) They have some nifty bonus features‚Äîlike type dispatch, potential reversability (to allow both encoding and <em>de</em>coding), and extensability to name a few‚Äîbut my primary use here is to use them as a medium for creating fast.ai <code>DataBlock</code>s in a convenient format for my partucular data. There are quite a few ways to define a <code>Transform</code>, but I opted for simply extending the <code>Transform</code> class. That looks like this:</p>
<pre><code class="language-python3">class SomeKindOfTransform(Transform): pass
</code></pre>
<p>But for it do actually do anything, we need the <code>Transform</code> to have an <code>encodes</code> method; so a transform that simply squares its input might look something like this:</p>
<pre><code class="language-python3">class SquareTransform(Transform):
    def encodes(self, x):
        return x**2

SquareTransform(12) # Gives us 144.
</code></pre>
<h3>Defining <code>NYearsTransform</code></h3>
<p>Alright! Now to define a <code>Transform</code> to accept two nodes and return an image representation of them based on their neighbors. First, we can to prepare all of the logic to handle the situations outlined above.</p>
<h4>Initialization, contextual settings, utilities</h4>
<p>Our <code>Transform</code> is going to need some background information for every transformation it does; things like:</p>
<ul>
<li>What are the dimensions of the final image?</li>
<li>What adjacency list are we pulling our information from?</li>
<li>How big is the <em>n</em> in &quot;<em>n</em> years,&quot; actually?</li>
</ul>
<p>as well as other utilities that may be used repeatedly in the transformation process. We'll these up to be incorporated when we initialize a <code>NYearsTransform</code> object using the <code>__init__</code> function:</p>
<pre><code class="language-python3">class NYearsTransform(Transform):

    # Initialization
    def __init__(self, adj_list, mode, img_shape=None, n=3):
        self.adj_list = adj_list # Attach an inputted adjacency list to the object
        if mode in {'train','eval'}: #Specify a mode (&quot;train&quot; or &quot;eval&quot;). We'll get to this in a moment!
            self.mode = mode
        else:
            raise ValueError('`mode` should be `&quot;train&quot;` or `&quot;eval&quot;`')

        # Time-related attributes
        self.t_delta = n * 365 # Graph times are in days, so we need to have &quot;n years&quot; in terms of days


        ### Latest time on the graph. (Avoids checking nodes with no connections.)
        max_graph_time = max(edges[1].max() for edges in adj_list.values() if len(edges))

        ### For training, only edges formed `n` years before the latest data are &quot;True&quot; values for certain:
        self.t_cutoff = max_graph_time - self.t_delta

        # Image-making attributes
        ### If you manually define a shape, set that as the shape...
        if img_shape is not None:
            self.img_shape = shape 
        ### Otherwise, image is the smallest square possible based on number of nodes
        else:
            len_sqrt = int(max(adj_list.keys())**0.5)
            self.img_shape = (len_sqrt, len_sqrt)

</code></pre>
<p>One important operation we'll need every time we use our transform is converting our list of neighboring nodes to an image, as <a href="#something-something">mentioned previously</a>. We can convert the same approach into a function and add it as a built-in part of our class:</p>
<pre><code class="language-python3">class NYearsTransform(Transform):
    def __init__(self, adj_list, mode, img_shape=None, n=3):
        self.adj_list = adj_list
        if mode in {'train','eval'}:
            self.mode = mode
        else:
            raise ValueError('`mode` should be `&quot;train&quot;` or `&quot;eval&quot;`')

        self.t_delta = n * 365

        max_graph_time = max(edges[1].max() for edges in adj_list.values() if len(edges))
        self.t_cutoff = max_graph_time - self.t_delta

        if img_shape is not None:
            self.img_shape = shape 
        else:
            len_sqrt = int(max(adj_list.keys())**0.5)
            self.img_shape = (len_sqrt, len_sqrt)

    # *NEW* Adding a utility function to turn list of neighbors to image tensors.
    def get_img(self, edges):
        base = torch.zeros(np.prod(self.img_shape),dtype=torch.float)
        base[edges] = 1
        return TensorImage(base).reshape(self.img_shape)
</code></pre>
<h4>Training vs. evaluation modes</h4>
<p>If you read my <a href="#Step-2">earlier exposition</a> carefully, you may have caught wind of the subtle intimation that we're going to need to do fairly different things during training vs. inference. To deal with this I decide to set the <code>Transform</code> up to handle two disting &quot;modes&quot;: <code>'train'</code>, where we carefully supply timeframes and contexts that are useful for training, and <code>'eval'</code>, where we just grab the most up-to-date connectivity information and go. I decided to just have the mode be determined by passing in a string.</p>
<p>This is where we finally get to define the full behavior of the <code>Transform</code>! This is done with the associated <code>encodes</code> function. It will take in a pair of nodes (a <code>nodepair</code>) and, based on the mode, return the appropriate image. If we're in &quot;Train&quot; mode, the transform will return not just the image, but also the label correctly answering the question, <em>Will these two nodes be connected in <em>n</em> years?</em></p>
<p>One bonus aspect we might want to consider is differentiating 1st and 2nd order neighbors in our image. It may be useful for the model to distinguish between those when trying to make a prediction. One way we could handle this is by having a (2 * <em>k</em>)-channel image; in other words, each set of <em>k</em>th order neighbors gets its own layer for each node. But this might make visualization a bit tricky (if we want to actually look at the &quot;images&quot;), because most libraries expect an image to either be 1-channel or 3-channel, and it also means larger tensors are getting passed in. Another option would be summing layers together in a way that retains the seperability of the information at the end, which is the approach I opted for. For example, if we weight first order neighbors as <code>0.75</code> and second order neighbors as <code>0.25</code>, then we can tell that a node with <code>0</code> had no connections, a node with <code>0.75</code> was exclusively a first order neighbor, one with <code>0.25</code> is exclusively a second order neighbor, and <code>1.0</code> is a node that is connected by both one <em>and</em> two hops. Here's the final implementation of the <code>Transform</code> including summed-and-weighted neighbor layers and both transform modes.</p>
<pre><code class="language-python3">class NYearsTransform(Transform):

    def __init__(self, adj_list, mode, img_shape=None, n=3):
        self.adj_list = adj_list
        if mode in {'train','eval'}:
            self.mode = mode # Told you we'd get there!
        else:
            raise ValueError('`mode` should be `&quot;train&quot;` or `&quot;eval&quot;`')

        self.t_delta = n * 365

        max_graph_time = max(edges[1].max() for edges in adj_list.values() if len(edges))
        self.t_cutoff = max_graph_time - self.t_delta

        if img_shape is not None:
            self.img_shape = shape 
        else:
            len_sqrt = int(max(adj_list.keys())**0.5) + 1
            self.img_shape = (len_sqrt, len_sqrt)

        # *NEW* Adding in weighting for 1st and 2nd order neighbors.
        self.weights = (0.75, 0.25)

    # *NEW* Defining our encodes!
    def encodes(self, nodepair:Iterable):
        nodepair = {int(n) for n in nodepair}

        ### EVAL ### (Returns just a TensorImage)
        if self.mode == 'eval':
            # Create, weigh, and sum layers for each node
            img_layers = [sum(self.get_img(layer)*weight
                              for layer,weight
                              in zip(get_neighbors(self.adj_list,n),self.weights))
                          for n in nodepair]

            # Return stacked representation of nodes.
            return torch.stack(img_layers)

        ### TRAIN ### (Train mode will return the TensorImage *and* a label.)
        elif self.mode == 'train':
            # To check for connectivity, we need to look at nodes individually for a sec.
            nodes_copy = nodepair.copy()
            link_list = arr_dict[nodes_copy.pop()]
            partner_node = nodes_copy.pop()
            linked = partner_node in link_list[0] # A boolean value

            # If nodes connect
            if linked:
                lim = link_list[1][link_list[0] == partner_node] # Time of the connection

                # Passing that time into `get_neighbors` gets all connections *before* that point
                img_layers = [sum(self.get_img(layer)*weight
                                  for layer,weight
                                  in zip(get_neighbors(self.adj_list,n,lim),self.weights))
                              for n in nodepair]

                # If either edgelist is empty at this time (i.e. this is &gt;= 1 node's first link ever)
                if any(layer.sum() == 0 for layer in img_layers):
                    label = 0

                # Otherwise (normal behavior):
                else:
                    partner_links = arr_dict[partner_node]

                    # Most recent link time before these two nodes linked:
                    maxlink = np.max([np.max(link_list[1][link_list[1] &lt; lim]),
                                      np.max(partner_links[1][partner_links[1] &lt; lim])])

                    # Verify previous link (image reference) and most recent link are actually within n years.
                    label = 1 if lim - maxlink &lt; self.t_delta else 0

            # If nodes do not connect
            else:
                # Get connections as they are at `self.t_cutoff` 
                # (any later and we aren't *really* sure they won't connect in n years
                img_layers = [sum(self.get_img(layer)*weight
                                  for layer,weight
                                  in zip(get_neighbors(self.adj_list,n,self.t_cutoff),self.weights))
                              for n in nodepair]

                label = 0

            # Return image,label
            return torch.stack(img_layers),label

    def get_img(self, edges):
        base = torch.zeros(np.prod(self.img_shape),dtype=torch.float)
        base[edges] = 1
        return TensorImage(base).reshape(self.img_shape)
</code></pre>
<p>Now that our transform is ready to go, we can flesh out the training pipeline, and get training!.</p>
<h3>Training pipeline</h3>
<p>So we have a <code>Transform</code> that converts to topics into image representations of those topics based on their connectivity, and will also provide a label telling us whether those images represent two graphs that will be connected in <em>n</em> years. But how do we use that to create a model? All we have to do is pass in a bunch of node pairs to the transform, and then pass the output of the transform (the image and label) to a vision model, and train!</p>
<p>Fast.ai uses <a href="#link-to-datablock-api">&quot;datablocks&quot;</a> for training‚Äîobjects containing either the data and labels themselves, or a way to <em>get</em> the data and labels. While we don't have a built-in <code>DataBlock</code> that's fit to use with our current setup, we can make training and validation <code>DataBlock</code>s fairly simply from our <code>Transform</code>. The built-in <code>ImageBlock</code>, for example, takes in strings indicating image file paths and labels, uses those to get the actual images/labels, and passes those to the model‚Äîbut doesn't actually store the images themselves <em>in</em> the datablock. We'll do something similar, but instead of getting images from a file, we're generating them based on simply inputting two node id's. Since we've built our own custom <code>Transform</code> that's not dependent upon an existing type of <code>DataBlock</code>, we'll use a <a href="https://docs.fast.ai/data.core.html#TfmdLists"><code>TfmdList</code></a> (&quot;Transformed List&quot;) instead of an actual <code>DataBlock</code> object, but they're fundamentally the same: take in information and transform it on-the-fly into the format needed for training. But before we make a <code>TfmdList</code>, we'll need to select pairs of nodes we want to use for training and for validation.</p>
<h4>Training and validation sets</h4>
<p>We could use a much larger training set, but to keep training time down, I'll start with something a bit more modest. Let's say‚Ä¶5000 samples? We could select 5000 random combinations of nodes for our training set, but remember that the dataset is <em>incredibly</em> sparse‚Äîso if we just pick node pairs as random, we'll end up with a <em>lot</em> of &quot;these topics do not connect&quot;, so the point where training might not even accomplish anything and the model may just assume that the correct label is always 0. A better idea is to try to intentionally balance our training set so that it has roughly equal amounts of positive and negative samples; cases where we know for sure that the nodes will connect, along with cases where they won't. So let's split our training set up into equal &quot;positive&quot; and &quot;negative&quot; groups‚Äîthe <code>TfmdList</code> will want these as lists, so we'll also make sure our sample node pairs are in that format:</p>
<pre><code class="language-python3">pos_size = 2500
neg_size = 2500

# Set up some random number generation in NumPy (with seed = 42)
rng = np.random.default_rng(42)

pos_size = 2500
neg_size = 2500

pos_sel = rng.choice(len(graph),pos_size,replace=False)
neg_sel = rng.choice(len(unconnected_v_pairs),neg_size,replace=False)

# Grab 2500 pairs that already exist from the graph (w/o time column)
pos_sample = list(graph[pos_sel,:2])

# Pull 2500 pairs from the `unconnected_v_list`, all of which are unconnected at this point
neg_sample = list(unconnected_v_pairs[neg_sel]) 
</code></pre>
<p>We'll also want to split up our samples so we have a both a training <em>and</em> a validation set. If we take the samples we've already generated (instead of generating more), we'll make sure we don't accidentally end up with duplicated pairs in both our training and validation sets (which would kind of defeat the purpose of having a validation set in the first place).</p>
<pre><code class="language-python3">split = int(2500*0.2) # Pick an index to split at

pos_train, pos_valid = pos_sample[:split],pos_sample[split:]
neg_train, neg_valid = neg_sample[:split],neg_sample[split:]

train_list = pos_train + neg_train
valid_list = pos_valid + neg_valid
</code></pre>
<h4>Transformed Lists, DataLoaders</h4>
<p>Now to turn the samples into <code>TfmdLists</code>, with the correct transform behavior depending on if we're training or validating:</p>
<pre><code class="language-python3">train_tl = TfmdLists(train_list, NYearsTransform(arr_dict,mode='train',n=3))
valid_tl = TfmdLists(valid_list, NYearsTransform(arr_dict,mode='eval',n=3))
</code></pre>
<p>Not a long step üòÅ</p>
<p>Now that we have our transformed lists, they function just like a fast.ai <code>Dataset</code> object, which means we can actually pass them straight into a <code>DataLoaders</code> and get training!</p>
<pre><code class="language-python3">dls = DataLoaders.from_dsets(train_tl, valid_tl)
dls = dls.cuda() # If you're using a GPU (probably a good idea)
</code></pre>
<h2>Training</h2>
<p>With all of that setup done, we can create our model/learner, give it some metrics, and let it rip! The competition's performance metric is ROC AUC, so it might be handy to use that as a metric while training. Fast.ai has this as a built-in metric already, so we can just initialize it and add it straight in to our learner:</p>
<pre><code class="language-python3">roc_auc = RocAucBinary()
graph_learner = cnn_learner(dls,
                            xse_resnext18, # You could slot in any vision model here and compare performance. I picked `xse_resnext18` at the time.
                            metrics=[roc_auc,accuracy],
                            pretrained=False,
                            n_in=2,n_out=2, # We have two &quot;channels&quot; (one for each node's image) to pass in, hence n_in=2. And it's binary classification, so n_out=2 also.
                            loss_func=CrossEntropyLossFlat())
</code></pre>
<p>And now, we train!</p>
<pre><code class="language-python3">graph_learner.fit_one_cycle(5) # Train for 5 epochs.
</code></pre>
<h2>Results; or, not everything is a vision problem.</h2>
<p>For brevity, I'll spare you the play-by-play process of validating the model against the solution set provided. The real question is, how did it do? At the end of training, the final ROC AUC score was...</p>
<pre><code>ROC_AUC: 0.7305075966006858
</code></pre>
<p>which is quite a bit lower than the tutorial solution's performance.</p>
<p>But that's okay! Not every approach is going to work for any given problem, and I got to dig a lot deeper into the fast.ai framework in the process of bringing the idea to fruition. Looking back over the setup and execution, I can also see some potential areas of improvement that might bring the model up to a more competitive level.</p>

</body>
</html>