<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="Cache-control" content="public">
  <meta name="theme-color" content="#ffbd16">
  <meta name="color-scheme" content="light dark">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/style.css">
  <!-- Grab title from the page data and dump it here -->
  <title>Eatiquette ingredient task overview | Jackson Mostoller</title>
</head>
<body>
  <nav id="navbar">
  <a class="logotype" href="/">Jackson Mostoller</a>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/notes/">Open Notes</a></li>
    <li><a href="https://github.com/mostol">GitHub<svg style="display: inline-block; width: 0.7em; height: 0.7em;" viewbox="0 0 48 48">
<path d="M36 24c-1.2 0-2 0.8-2 2v12c0 1.2-0.8 2-2 2h-22c-1.2 0-2-0.8-2-2v-22c0-1.2 0.8-2 2-2h12c1.2 0 2-0.8 2-2s-0.8-2-2-2h-12c-3.4 0-6 2.6-6 6v22c0 3.4 2.6 6 6 6h22c3.4 0 6-2.6 6-6v-12c0-1.2-0.8-2-2-2z"></path>
<path d="M43.8 5.2c-0.2-0.4-0.6-0.8-1-1-0.2-0.2-0.6-0.2-0.8-0.2h-12c-1.2 0-2 0.8-2 2s0.8 2 2 2h7.2l-18.6 18.6c-0.8 0.8-0.8 2 0 2.8 0.4 0.4 0.8 0.6 1.4 0.6s1-0.2 1.4-0.6l18.6-18.6v7.2c0 1.2 0.8 2 2 2s2-0.8 2-2v-12c0-0.2 0-0.6-0.2-0.8z"></path>
</svg>
</a></li>
    </li>
  </ul>
</nav>
  <!-- Grab the content from the page data and dump it here, and mark it as safe -->
  <!-- Safe docs: https://mozilla.github.io/nunjucks/templating.html#safe -->
  <main>
  <h1>Eatiquette ingredient task overview</h1><div class="cblock">
With the massive proliferation of machine learning techniques, there are probably infinite possible approaches to the ingredient-mapping task. After some consideration, I feel that some kind of transformer-based approach might turn out to be among the easiest to implement and most elegant.
<p>Transformer-centric models are doing incredible things right now, and it wouldn't be all that difficult to get a transformer up-and-running that could accomplish this labelling task with a good amount of accuracy. But what would that look like?</p>
<p>My inital instict would perhaps be to look at some kind of classifier, i.e. &quot;tomato sauce&quot; and &quot;tomato paste&quot; both get classified as <code>tomato</code>, &quot;baking soda&quot; and &quot;SODIUM BICARBONATE&quot; are classified as <code>sodium bicarbonate</code>, and so forth. The issue with this approach is that we're looking at <em>at least</em> 3000-4000 &quot;classes&quot;—which would mean we would need at least a few examples of sometimes-very-rare ingredients, because instrisically a class' label is meaningless, so we would need concrete examples in each instance.</p>
</div>
<h2>A Sequence-to-sequence approach</h2>
<div class="cblock">
A more promising option that would require less input data and far-fetched samples would be to create a sequence-to-sequence model that is, in effect, a "summarizer"—taking in full/long versions of ingredients and returning their summed-up or essentialized version. Just like article-summarization models, this kind of micro-summarizer should be able to tackle new, unseen inputs without a problem. Here's how this kind of model would handle the three components outlined in the task description:
<ol>
<li><em>Identify core ingredients.</em> The seq-to-seq model would be trained to convert a non-standardized ingredient into a standardized format (e.g. &quot;whole wheat flour&quot; to <code>wheat</code>). The core ingredients list would then consist of the set of all outputs from the model for every one of the 300,000+ ingredients in the non-standardized list, which should be able to convert any given ingredient-like string into its &quot;core&quot; ingredient (since that's what it's been trained to do ☺️).</li>
<li><em>Map ingredients to the list of core ingredients.</em> Step 2 is completed in essentially the same breath as step one: the model learns to map an ingredient to its &quot;core&quot; version, and the total output of all ingredients forms the canonical &quot;core&quot; list.</li>
<li><em>How will the model work for a new set of ingredients?</em> The model should have been trained without seeing <em>all</em> ingredient examples (i.e. using a validation set), so if it had been trained correctly it will have properly generalized the notion of &quot;tell me the core ingredient of this nonstandard ingredient listing,&quot; and will be able to do so for ingredients that it has not yet seen. This should especially be feasible if the model is based on an existing pre-trained model, which will still be familiar with concepts and vocabulary beyond the scope of the training set and can generalize this preexisting knowledge to tackle never-before-seen ingredients.</li>
</ol>
<p>Using a pre-trained transformer-based language mode allows us to leverage highly sophisticated linguistic connections between words, and also enables high generalizability which helps the model handle data issues such as spelling errors or irrelevant information (like extended but unnecessary marketing descriptions of ingredients).</p>
</div>
<h2>Some considerations</h2>
<div class="cblock">
While this approach seems to check all of the boxes, there are a few considerations to tak into account, as there would be with nearly any approach. One aspect of this kind of transformer-based sequence-to-sequence approach is the need for labelled training data, which means that 1) up to several thousand examples may need to be labelled by hand before the model can function correctly, and 2) because this model runs on labelled data in a supervised fashion, certain design decisions need to be made before hand and become somewhat inflexible once the model is trained. For example, what do we want to count as an "ingredient"? How different are "soy" and "soy sauce"? What about "beef" and "beef broth"? If there are multiple ingredients in one input, should we be able to detect that and reduce things to multiple core ingredients? With this kind of supervised learning approach, we need to make these calls at the beginning—which is fine!—but if we want to change our decisions later we may have to do some re-training or re-evaluating. 
<p>There are upsides to these ramifcations as well, however. For example, the task description notes that &quot;Process identifiers from an ingredient like ‘dried’, ‘roasted’, ‘nonGMO’, ‘gluten-free’ should be treated separately.&quot; We could take this to mean that &quot;dried&quot;, &quot;roasted&quot;, &quot;non-GMO&quot;, and &quot;gluten-free&quot; are to be <em>unconsidered</em> or <em>ignored</em> by the model, because we only want the ingredients. But, aligning with the Eatiquette matra:</p>
<blockquote>
<p>Food taste, preference, and medical avoidance is highly individual. We are not here to push for a certain lifestyle and try to stay away from opinions on what you should and should not eat.</p>
</blockquote>
<p>Some of these pieces of information may be useful to keep track of, even if not necessarily mandated by the current task constraints. Users may, for example, want to know about organic ingredients, or their geographic origins, or whether something is whole-grain, or even an ingredient's preparation method. Since the precise way in which data is labelled is important and will require meaningful forethought, we could take this opportunity to label the data and train the model to extract as much meaningful information as possible and open up new possibilities.</p>
</div>
  </main>
</body>
</html>